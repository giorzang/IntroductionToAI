{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d20a960",
   "metadata": {},
   "source": [
    "###  Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64065527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47803420",
   "metadata": {},
   "source": [
    "### Knapsack Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06cfb434",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KnapsackEnv:\n",
    "    def __init__(self, n_items=10, weights=[], values=[], capacity=300):\n",
    "        self.n = n_items\n",
    "        self.weights = weights\n",
    "        self.values = values\n",
    "        self.capacity = capacity # np.random.randint(1, sum(self.weights))\n",
    "\n",
    "    def reset(self):\n",
    "        self.selected = np.zeros(self.n, dtype=int)\n",
    "        self.rem_w = self.capacity\n",
    "        total_value = self.values.sum()\n",
    "\n",
    "        # Ma trận state (n+1 items, 3 features) [selected, value, weight]\n",
    "        self.state = np.zeros((self.n+1, 3), dtype=float)\n",
    "        for i in range(self.n):\n",
    "            self.state[i] = [\n",
    "                0,\n",
    "                self.values[i] / total_value,\n",
    "                self.weights[i] / self.capacity\n",
    "            ]\n",
    "        # Hàng cuối: capacity còn lại (ban đầu normalized = 1)\n",
    "        self.state[self.n] = [0, 1.0, 1.0]\n",
    "\n",
    "        return self.state.flatten().astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        i = action\n",
    "        reward = 0.0\n",
    "        done = False\n",
    "\n",
    "        # check valid\n",
    "        if self.selected[i] == 0 and self.weights[i] <= self.rem_w:\n",
    "            self.selected[i] = 1\n",
    "            self.rem_w -= self.weights[i]\n",
    "            reward = float(self.values[i])\n",
    "\n",
    "            # update flag selected\n",
    "            self.state[i, 0] = 1.0\n",
    "            # update remaining capacity\n",
    "            self.state[self.n] = [0, 0.0, self.rem_w / self.capacity]\n",
    "        else:\n",
    "            reward = 0.0\n",
    "\n",
    "        # check done\n",
    "        feasible = False\n",
    "        for j in range(self.n):\n",
    "            if self.selected[j] == 0 and self.weights[j] <= self.rem_w:\n",
    "                feasible = True\n",
    "                break\n",
    "        if not feasible:\n",
    "            done = True\n",
    "\n",
    "        return self.state.flatten().astype(np.float32), reward, done, {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1658af17",
   "metadata": {},
   "source": [
    "### Select action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "589cee80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(env, state, q_net, epsilon):\n",
    "    # epsilon-greedy exploration\n",
    "    if random.random() < epsilon:\n",
    "        # explore: chọn ngẫu nhiên trong các action hợp lệ\n",
    "        valid_actions = []\n",
    "        for i in range(env.n):\n",
    "            if env.selected[i] == 0 and env.weights[i] <= env.rem_w:\n",
    "                valid_actions.append(i)\n",
    "        if valid_actions:\n",
    "            return random.choice(valid_actions)\n",
    "        else:\n",
    "            return None # không có action hợp lệ\n",
    "    else:\n",
    "        # exploit: chọn action có Q lớn nhất\n",
    "        state_t = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "        q_values = q_net(state_t).detach().cpu().numpy().squeeze()\n",
    "\n",
    "        # mask: True neu item hop le\n",
    "        mask = np.zeros(env.n, dtype=bool)\n",
    "        for i in range(env.n):\n",
    "            if env.selected[i] == 0 and env.weights[i] <= env.rem_w:\n",
    "                mask[i] = True\n",
    "\n",
    "        # neu khong con action hop le\n",
    "        if not mask.any():\n",
    "            return None\n",
    "        q_values[~mask] = -1e9\n",
    "\n",
    "        # chon action co Q lon nhat\n",
    "        action = int(np.argmax(q_values))\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e7fec8",
   "metadata": {},
   "source": [
    "### Dueling DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c310f450",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuelingDQN(nn.Module):\n",
    "    def __init__(self, input_dim, n_actions, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        # phần chung (feature extractor)\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        # value stream\n",
    "        self.v_fc = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.v_out = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "        # advantage stream\n",
    "        self.a_fc = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.a_out = nn.Linear(hidden_dim, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "\n",
    "        # value branch\n",
    "        v = F.relu(self.v_fc(x))\n",
    "        v = self.v_out(v)  # shape: (batch, 1)\n",
    "\n",
    "        # advantage branch\n",
    "        a = F.relu(self.a_fc(x))\n",
    "        a = self.a_out(a)  # shape: (batch, n_actions)\n",
    "\n",
    "        # combine theo công thức dueling\n",
    "        q = v + a - a.mean(dim=1, keepdim=True)\n",
    "        return q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e1df62",
   "metadata": {},
   "source": [
    "### Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2571a3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=100000):\n",
    "        self.buffer = collections.deque(maxlen=capacity)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return (\n",
    "            np.array(states),\n",
    "            np.array(actions),\n",
    "            np.array(rewards, dtype=np.float32),\n",
    "            np.array(next_states),\n",
    "            np.array(dones, dtype=np.float32),\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f5dd43",
   "metadata": {},
   "source": [
    "### Train step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e30c3883",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(online_net, target_net, buffer, optimizer, batch_size=64, gamma=0.99):\n",
    "    if len(buffer) < batch_size:\n",
    "        return None  # chưa đủ dữ liệu để train\n",
    "\n",
    "    # Sample minibatch\n",
    "    states, actions, rewards, next_states, dones = buffer.sample(batch_size)\n",
    "\n",
    "    # Convert sang tensor\n",
    "    states = torch.tensor(states, dtype=torch.float32).to(device)\n",
    "    actions = torch.tensor(actions, dtype=torch.long).to(device)\n",
    "    rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
    "    next_states = torch.tensor(next_states, dtype=torch.float32).to(device)\n",
    "    dones = torch.tensor(dones, dtype=torch.float32).to(device)\n",
    "\n",
    "    # Q-values cho actions đã chọn\n",
    "    q_values = online_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "    # Double DQN target\n",
    "    with torch.no_grad():\n",
    "        next_actions = online_net(next_states).argmax(dim=1)  # chọn action bằng online_net\n",
    "        next_q = target_net(next_states).gather(1, next_actions.unsqueeze(1)).squeeze(1)\n",
    "        target = rewards + gamma * next_q * (1 - dones)\n",
    "\n",
    "    # Loss = MSE\n",
    "    loss = F.mse_loss(q_values, target)\n",
    "\n",
    "    # Backprop\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc09e73",
   "metadata": {},
   "source": [
    "### Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4150f35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def knapsack_dp(weights, values, capacity):\n",
    "    n = len(weights)\n",
    "    dp = [0] * (capacity + 1)\n",
    "    for i in range(n):\n",
    "        w, v = weights[i], values[i]\n",
    "        for cap in range(capacity, w-1, -1):\n",
    "            dp[cap] = max(dp[cap], dp[cap-w] + v)\n",
    "    return max(dp)\n",
    "\n",
    "def knapsack_greedy(weights, values, capacity):\n",
    "    n = len(weights)\n",
    "    ratio = [v / w for w, v in zip(weights, values)]\n",
    "    indices = sorted(range(n), key=lambda k: ratio[k], reverse=True)\n",
    "    total_value = 0\n",
    "    total_weight = 0\n",
    "    for i in indices:\n",
    "        if total_weight + weights[i] <= capacity:\n",
    "            total_value += values[i]\n",
    "            total_weight += weights[i]\n",
    "    return total_value\n",
    "\n",
    "def evaluate_graph(env, model, n_eval=100):\n",
    "    dp_results = []\n",
    "    greedy_results = []\n",
    "    agent_results = []\n",
    "    for _ in tqdm(range(n_eval), desc=\"Evaluating\"):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            action = select_action(env, state, model, epsilon=0)\n",
    "            if action is None:\n",
    "                break\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "        dp_result = knapsack_dp(env.weights, env.values, env.capacity)\n",
    "        greedy_result = knapsack_greedy(env.weights, env.values, env.capacity)\n",
    "        dp_results.append(dp_result)\n",
    "        greedy_results.append(greedy_result)\n",
    "        agent_results.append(total_reward)\n",
    "    return dp_results, greedy_results, agent_results\n",
    "\n",
    "def evaluate_agent(env, model, n_eval=100):\n",
    "    accuracies = []\n",
    "    for _ in range(n_eval):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            action = select_action(env, state, model, epsilon=0)\n",
    "            if action is None:\n",
    "                break\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "        optimal = knapsack_dp(env.weights, env.values, env.capacity)\n",
    "        acc = total_reward / (optimal + 1e-9)\n",
    "        accuracies.append(acc)\n",
    "    return np.mean(accuracies), np.std(accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a39c639",
   "metadata": {},
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/10 [00:00<?, ?it/s]/workspaces/IntroductionToAI/env/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:192: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "Training: 100%|██████████| 10/10 [11:48<00:00, 70.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy: 0.9691064670722953\n",
      "Std accuracy: 0.0\n",
      "\n",
      "Training done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "all_mean_acc = []\n",
    "all_std_acc = []\n",
    "all_weights = []\n",
    "all_values = []\n",
    "all_capacity = []\n",
    "all_online_net = []\n",
    "\n",
    "for sample in tqdm(range(10), desc=\"Training\"):\n",
    "    n_items = 10\n",
    "    episodes = 5000        # để test, bài báo dùng vài ngàn\n",
    "    buffer_capacity = 50000\n",
    "    batch_size = 64\n",
    "    gamma = 0.99\n",
    "    lr = 1e-3\n",
    "    # target_update_freq = 100\n",
    "    epsilon_start = 1.0\n",
    "    epsilon_end = 0.01\n",
    "    epsilon_decay = 0.9985  # decay factor\n",
    "\n",
    "    # Sample data\n",
    "    weights = np.random.randint(1, 100, n_items)\n",
    "    values = np.random.randint(1, 100, n_items)\n",
    "    capacity = np.random.randint(1, sum(weights))\n",
    "\n",
    "    all_weights.append(weights)\n",
    "    all_values.append(values)\n",
    "    all_capacity.append(capacity)\n",
    "\n",
    "    # Env + networks\n",
    "    env = KnapsackEnv(n_items=n_items, weights=weights, values=values, capacity=capacity)\n",
    "    input_dim = (n_items+1) * 3\n",
    "    n_actions = n_items\n",
    "\n",
    "    online_net = DuelingDQN(input_dim, n_actions, hidden_dim=64).to(device)\n",
    "    target_net = DuelingDQN(input_dim, n_actions, hidden_dim=64).to(device)\n",
    "    target_net.load_state_dict(online_net.state_dict())\n",
    "\n",
    "    optimizer = optim.Adam(online_net.parameters(), lr=lr)\n",
    "    scheduler = StepLR(optimizer, step_size=250, gamma=0.9)\n",
    "    buffer = ReplayBuffer(capacity=buffer_capacity)\n",
    "\n",
    "    epsilon = epsilon_start\n",
    "\n",
    "    # Training\n",
    "    all_rewards = []\n",
    "    for ep in range(episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            action = select_action(env, state, online_net, epsilon)\n",
    "            if action is None:\n",
    "                break\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            buffer.add(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "            # train step\n",
    "            loss = train_step(online_net, target_net, buffer, optimizer,\n",
    "                            batch_size=batch_size, gamma=gamma)\n",
    "\n",
    "        all_rewards.append(total_reward)\n",
    "        scheduler.step()\n",
    "        target_net.load_state_dict(online_net.state_dict())\n",
    "\n",
    "        # Epsilon decay\n",
    "        epsilon = max(epsilon_end, epsilon * epsilon_decay)\n",
    "    all_online_net.append(online_net)\n",
    "\n",
    "for _ in range(10):\n",
    "    weights = all_weights[_]\n",
    "    values = all_values[_]\n",
    "    capacity = all_capacity[_]\n",
    "    online_net = all_online_net[_]\n",
    "    env = KnapsackEnv(n_items=n_items, weights=weights, values=values, capacity=capacity)\n",
    "\n",
    "    mean_acc, std_acc = evaluate_agent(env, online_net, n_eval=1)\n",
    "    all_mean_acc.append(mean_acc)\n",
    "    all_std_acc.append(std_acc)\n",
    "print(\"Mean accuracy:\", np.mean(all_mean_acc))\n",
    "print(\"Std accuracy:\", np.mean(all_std_acc))\n",
    "print(\"\\nTraining done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01afd826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy: 0.6916363636351062\n",
      "Std accuracy: 0.22600109704951182\n"
     ]
    }
   ],
   "source": [
    "accuracies = []\n",
    "for _ in range(10):\n",
    "    weights = all_weights[_]\n",
    "    values = all_values[_]\n",
    "    capacity = all_capacity[_]\n",
    "    optimal = knapsack_dp(env.weights, env.values, env.capacity)\n",
    "    total_reward = knapsack_greedy(weights, values, capacity)\n",
    "    \n",
    "    acc = total_reward / (optimal + 1e-9)\n",
    "    accuracies.append(acc)\n",
    "\n",
    "print(\"Mean accuracy:\", np.mean(accuracies))\n",
    "print(\"Std accuracy:\", np.std(accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c675925a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import math\n",
    "\n",
    "# epsilon_start = 1.0\n",
    "# epsilon_end = 0.01\n",
    "# epsilon_mult = 0.9993  # decay factor mỗi episode\n",
    "\n",
    "# t = math.log(epsilon_end / epsilon_start) / math.log(epsilon_mult)\n",
    "# print(f\"Epsilon đạt {epsilon_end} ở episode ≈ {t:.1f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
